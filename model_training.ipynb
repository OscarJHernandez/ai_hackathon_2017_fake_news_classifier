{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Number:  400\n",
      "Testing Set Number:  100\n",
      "==============================================================\n",
      "The Size of the dictionary 2515\n",
      "Top Dictionary Words:  [('the', 48), ('trump', 23), ('in', 15), ('to', 14), ('us', 14), ('clinton', 13), ('hillary', 11), ('neil', 11), ('for', 10), ('young', 10)]\n",
      "\n",
      "==============================================================\n",
      "array of labels size:  400\n",
      "feature matrix:  400\n",
      "=========================================\n",
      "Feature Matrix computation completed\n",
      "=========================================\n",
      "========================================================================================================================\n",
      "Fitting will now take place\n",
      "========================================================================================================================\n",
      "========================================================================================================================\n",
      "Fitting of Models is Complete\n",
      "========================================================================================================================\n",
      "========================================================================================================================\n",
      "Model Predictions on Test Set Complete\n",
      "========================================================================================================================\n",
      "======================================\n",
      "Unormalized C1 [[40  6]\n",
      " [19 35]] \n",
      "\n",
      "Unormalized C2 [[41  5]\n",
      " [20 34]] \n",
      "\n",
      "Unormalized C3 [[45  1]\n",
      " [44 10]] \n",
      "\n",
      "Unormalized C3_DecTree [[45  1]\n",
      " [42 12]] \n",
      "\n",
      "Unormalized C3_Random_Forest [[46  0]\n",
      " [45  9]] \n",
      "\n",
      "Unormalized C3_Extra_Tree [[45  1]\n",
      " [43 11]] \n",
      "\n",
      "Unormalized C4 [[45  1]\n",
      " [46  8]] \n",
      "\n",
      "Unormalized C4_AdaBoost [[42  4]\n",
      " [32 22]] \n",
      "\n",
      "Unormalized C4_gradiantBoost [[42  4]\n",
      " [33 21]] \n",
      "\n",
      "Unormalized C4_KNearestN [[45  1]\n",
      " [51  3]] \n",
      "\n",
      "Unormalized C5_Voting [[43  3]\n",
      " [38 16]] \n",
      "\n",
      "Unormalized C6_NB_Gauss [[33 13]\n",
      " [10 44]] \n",
      "\n",
      "Unormalized C7 NB_Bernulli [[42  4]\n",
      " [24 30]] \n",
      "\n",
      "Unormalized C8 NB_Ensemble [[33 13]\n",
      " [11 43]] \n",
      "\n",
      "======================================\n",
      "======================================\n",
      "Normalized C1 [[ 0.86956522  0.13043478]\n",
      " [ 0.35185185  0.64814815]] \n",
      "\n",
      "Normalized C2 [[ 0.89130435  0.10869565]\n",
      " [ 0.37037037  0.62962963]] \n",
      "\n",
      "Normalized C3 [[ 0.97826087  0.02173913]\n",
      " [ 0.81481481  0.18518519]] \n",
      "\n",
      "Normalized C3_DecTree [[ 0.97826087  0.02173913]\n",
      " [ 0.77777778  0.22222222]] \n",
      "\n",
      "Normalized C3_Random_Forest [[ 1.          0.        ]\n",
      " [ 0.83333333  0.16666667]] \n",
      "\n",
      "Normalized C3_Extra_Tree [[ 0.97826087  0.02173913]\n",
      " [ 0.7962963   0.2037037 ]] \n",
      "\n",
      "Normalized C4 [[ 0.97826087  0.02173913]\n",
      " [ 0.85185185  0.14814815]] \n",
      "\n",
      "Normalized C4_AdaBoost [[ 0.91304348  0.08695652]\n",
      " [ 0.59259259  0.40740741]] \n",
      "\n",
      "Normalized C4_gradiantBoost [[ 0.91304348  0.08695652]\n",
      " [ 0.61111111  0.38888889]] \n",
      "\n",
      "Normalized C4_KNearestN [[ 0.97826087  0.02173913]\n",
      " [ 0.94444444  0.05555556]] \n",
      "\n",
      "Normalized C5 Voting [[ 0.93478261  0.06521739]\n",
      " [ 0.7037037   0.2962963 ]] \n",
      "\n",
      "Unormalized C6_NB_Gauss [[ 0.7173913   0.2826087 ]\n",
      " [ 0.18518519  0.81481481]] \n",
      "\n",
      "Unormalized C7 NB_Bernulli [[ 0.91304348  0.08695652]\n",
      " [ 0.44444444  0.55555556]] \n",
      "\n",
      "Unormalized C8 NB_Ensemble [[ 0.7173913  0.2826087]\n",
      " [ 0.2037037  0.7962963]] \n",
      "\n",
      "======================================\n",
      "=====================================================================================\n",
      "cm arrays saved to file\n",
      "pkl files saved to file\n",
      "Program Complete\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "#import sys \n",
    "#reload(sys) \n",
    "#sys.setdefaultencoding('utf8')\n",
    "import cPickle\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import numerical_routines as nr\n",
    "import collections \n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "# Import a lot of models...\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import random\n",
    "\n",
    "# Import Tree Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# The size of the dictionary array vectors\n",
    "dictionary_size = 3000\n",
    "data_set_size = 500 # Take a subset of the data, max size = 23,723\n",
    "split_frac = 0.8 # The parameter that sets the train/test set size\n",
    "\n",
    "# Here we read in the data-set of real and fake news items\n",
    "real_and_fake_df = pd.read_csv('data_file/fake_and_real_news_titles.csv')\n",
    "real_and_fake_df.head()\n",
    "\n",
    "# Take a subset of the data\n",
    "real_and_fake_df = real_and_fake_df[0:data_set_size]\n",
    "\n",
    "\n",
    "# Now we split the data into the training set and test data set\n",
    "data_set_size = len(real_and_fake_df)\n",
    "split_indx = int(data_set_size*split_frac)\n",
    "\n",
    "training_df = real_and_fake_df[:split_indx]\n",
    "testing_df = real_and_fake_df[split_indx:]\n",
    "\n",
    "training_size = len(training_df)\n",
    "testing_size = len(testing_df)\n",
    "\n",
    "print 'Training Set Number: ', training_size\n",
    "print 'Testing Set Number: ', testing_size\n",
    "\n",
    "# Now the data from the story titles is extracted into one large array containing all words in the title\n",
    "title_words_array = nr.aggregate_data(training_df.dropna(subset=['title']),size= training_size)\n",
    "#print title_words_array\n",
    "# Now we make filter the title words \n",
    "filtered_title_words_array = nr.create_dictionary(title_words_array)\n",
    "\n",
    "\n",
    "# Now we count the frequency of the different words\n",
    "dictionary = collections.Counter(filtered_title_words_array)\n",
    "\n",
    "list_to_remove = dictionary.keys()\n",
    "\n",
    "# Remove one character items \n",
    "for key in list_to_remove:\n",
    "    \n",
    "    if len(key) == 1:\n",
    "        del dictionary[key]\n",
    "\n",
    "# Now take a reasonable subset of the data\n",
    "dictionary = dictionary.most_common(dictionary_size)\n",
    "\n",
    "print '=============================================================='\n",
    "print 'The Size of the dictionary', len(filtered_title_words_array)\n",
    "print 'Top Dictionary Words: ', dictionary[0:10] \n",
    "print ''\n",
    "print '=============================================================='\n",
    "\n",
    "# We convert a data set into a feature matrix\n",
    "feature_matrix = nr.create_featureMatrix(training_df,dictionary,size= training_size)\n",
    "\n",
    "# We find the labels of the data that we converted into the feature matrix and convert into an array\n",
    "label_array = nr.create_label_array(training_df,size= training_size)\n",
    "\n",
    "print 'array of labels size: ', len(label_array)\n",
    "print 'feature matrix: ', len(feature_matrix)\n",
    "\n",
    "print '========================================='\n",
    "print 'Feature Matrix computation completed'\n",
    "print '========================================='\n",
    "\n",
    "\n",
    "\n",
    "# Now we train the various classifier\n",
    "model1 = MultinomialNB()\n",
    "model2 = LinearSVC()\n",
    "\n",
    "# Model\n",
    "m_DecTree = DecisionTreeClassifier(max_depth=5)\n",
    "m_RandForest = RandomForestClassifier(max_depth=5, n_estimators=50)\n",
    "m_ExTree = ExtraTreesClassifier(max_depth=5, n_estimators=50)\n",
    "\n",
    "\n",
    "print '========================================================================================================================'\n",
    "print 'Fitting will now take place'\n",
    "print '========================================================================================================================'\n",
    "\n",
    "\n",
    "model1.fit(feature_matrix,label_array)\n",
    "model2.fit(feature_matrix,label_array)\n",
    "mixingWeights = [1.0,1.0,1.0]\n",
    "numOfNeighs = 1 \n",
    "\n",
    "#model3 = VotingClassifier(estimators=[ ('dtc', clf1), ('rfc', clf2), ('etc', clf3) ], voting='soft', weights= mixingWeights )\n",
    "model3 = VotingClassifier(estimators=[ ('dtc', m_DecTree), ('rfc', m_RandForest), ('etc', m_ExTree) ], voting='soft', weights= mixingWeights )\n",
    "model3.fit(feature_matrix, label_array)\n",
    "\n",
    "# fit models individually\n",
    "m_DecTree.fit(feature_matrix, label_array)\n",
    "m_RandForest.fit(feature_matrix, label_array)\n",
    "m_ExTree.fit(feature_matrix, label_array)\n",
    "\n",
    "# More Models\n",
    "#clf4 = AdaBoostClassifier(n_estimators=50)\n",
    "#clf5 = GradientBoostingClassifier(n_estimators=50)\n",
    "#clf6 = KNeighborsClassifier(n_neighbors=numOfNeighs)\n",
    "\n",
    "# More Models\n",
    "m_AdaBoost = AdaBoostClassifier(n_estimators=50)\n",
    "m_gradBoost = GradientBoostingClassifier(n_estimators=50)\n",
    "m_KNneigh = KNeighborsClassifier(n_neighbors=numOfNeighs)\n",
    "\n",
    "model4 = VotingClassifier(estimators=[ ('dtc', m_AdaBoost), ('rfc', m_gradBoost), ('etc', m_KNneigh) ], voting='soft', weights= mixingWeights )\n",
    "model4.fit(feature_matrix, label_array)\n",
    "\n",
    "estimator_models = [ ('dtc', m_DecTree), ('rfc', m_RandForest), ('etc', m_ExTree),('dtc', m_AdaBoost), ('rfc', m_gradBoost), ('etc', m_KNneigh),\n",
    "                   ('MNB',model1)]\n",
    "model5 = VotingClassifier(estimators=estimator_models, voting='hard')\n",
    "model5.fit(feature_matrix, label_array)\n",
    "\n",
    "\n",
    "model6 = GaussianNB()\n",
    "model7 = BernoulliNB()\n",
    "\n",
    "model6.fit(feature_matrix,label_array)\n",
    "model7.fit(feature_matrix,label_array)\n",
    "\n",
    "\n",
    "estimator_models = [ ('m1', model1), ('m6', model6), ('m7', model7)]\n",
    "model8 = VotingClassifier(estimators=estimator_models, voting='soft')\n",
    "model8.fit(feature_matrix, label_array)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit the extra models\n",
    "m_AdaBoost.fit(feature_matrix, label_array)\n",
    "m_gradBoost.fit(feature_matrix, label_array)\n",
    "m_KNneigh.fit(feature_matrix, label_array)\n",
    "\n",
    "print '========================================================================================================================'\n",
    "print 'Fitting of Models is Complete'\n",
    "print '========================================================================================================================'\n",
    "\n",
    "#========================================================================================================================\n",
    "#========================================================================================================================\n",
    "# Now we make a prediction on the testing data set\n",
    "feature_matrix_test = nr.create_featureMatrix(testing_df,dictionary, size=testing_size)\n",
    "label_array_test = nr.create_label_array(testing_df,size= testing_size)\n",
    "\n",
    "# Calculate the results of all of the models\n",
    "result1 = model1.predict(feature_matrix_test)\n",
    "result2 = model2.predict(feature_matrix_test)\n",
    "\n",
    "# Make Predictions with the second ensemble models\n",
    "\n",
    "\n",
    "# Make Predictions with the third ensemble models\n",
    "result3 = model3.predict(feature_matrix_test)\n",
    "result3_1 = m_DecTree.predict(feature_matrix_test)\n",
    "result3_2 = m_RandForest.predict(feature_matrix_test)\n",
    "result3_3 = m_ExTree.predict(feature_matrix_test)\n",
    "\n",
    "result4 = model4.predict(feature_matrix_test)\n",
    "result4_1 = m_AdaBoost.predict(feature_matrix_test)\n",
    "result4_2 = m_gradBoost.predict(feature_matrix_test)\n",
    "result4_3 = m_KNneigh.predict(feature_matrix_test)\n",
    "\n",
    "result5 = model5.predict(feature_matrix_test)\n",
    "\n",
    "result6 = model6.predict(feature_matrix_test)\n",
    "result7 = model7.predict(feature_matrix_test)\n",
    "result8 = model8.predict(feature_matrix_test)\n",
    "\n",
    "print '========================================================================================================================'\n",
    "print 'Model Predictions on Test Set Complete'\n",
    "print '========================================================================================================================'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cm1 = confusion_matrix(label_array_test,result1)\n",
    "cm2 = confusion_matrix(label_array_test,result2)\n",
    "cm3 = confusion_matrix(label_array_test,result3)\n",
    "cm3_1 = confusion_matrix(label_array_test,result3_1)\n",
    "cm3_2 = confusion_matrix(label_array_test,result3_2)\n",
    "cm3_3 = confusion_matrix(label_array_test,result3_3)\n",
    "\n",
    "cm4 = confusion_matrix(label_array_test,result4)\n",
    "cm4_1 = confusion_matrix(label_array_test,result4_1)\n",
    "cm4_2 = confusion_matrix(label_array_test,result4_2)\n",
    "cm4_3 = confusion_matrix(label_array_test,result4_3)\n",
    "cm5 = confusion_matrix(label_array_test,result5)\n",
    "\n",
    "cm6 = confusion_matrix(label_array_test,result6)\n",
    "cm7 = confusion_matrix(label_array_test,result7)\n",
    "cm8 = confusion_matrix(label_array_test,result8)\n",
    "\n",
    "print '======================================'\n",
    "print 'Unormalized C1', cm1,'\\n'\n",
    "print 'Unormalized C2', cm2,'\\n'\n",
    "print 'Unormalized C3', cm3,'\\n'\n",
    "print 'Unormalized C3_DecTree', cm3_1,'\\n'\n",
    "print 'Unormalized C3_Random_Forest', cm3_2,'\\n'\n",
    "print 'Unormalized C3_Extra_Tree', cm3_3,'\\n'\n",
    "print 'Unormalized C4', cm4,'\\n'\n",
    "print 'Unormalized C4_AdaBoost', cm4_1,'\\n'\n",
    "print 'Unormalized C4_gradiantBoost', cm4_2,'\\n'\n",
    "print 'Unormalized C4_KNearestN',cm4_3,'\\n'\n",
    "print 'Unormalized C5_Voting',cm5,'\\n'\n",
    "print 'Unormalized C6_NB_Gauss',cm6,'\\n'\n",
    "print 'Unormalized C7 NB_Bernulli',cm7,'\\n'\n",
    "print 'Unormalized C8 NB_Ensemble',cm8,'\\n'\n",
    "print '======================================'\n",
    "\n",
    "# Now we print off the Confusion matrix for the models\n",
    "cm1 = cm1.astype('float') / cm1.sum(axis=1)[:, np.newaxis]\n",
    "cm2 = cm2.astype('float') / cm2.sum(axis=1)[:, np.newaxis]\n",
    "cm3 = cm3.astype('float') / cm3.sum(axis=1)[:, np.newaxis]\n",
    "cm3_1 = cm3_1.astype('float') / cm3_1.sum(axis=1)[:, np.newaxis]\n",
    "cm3_2 = cm3_2.astype('float') / cm3_2.sum(axis=1)[:, np.newaxis]\n",
    "cm3_3 = cm3_3.astype('float') / cm3_3.sum(axis=1)[:, np.newaxis]\n",
    "cm4 = cm4.astype('float') / cm4.sum(axis=1)[:, np.newaxis]\n",
    "cm4_1 = cm4_1.astype('float') / cm4_1.sum(axis=1)[:, np.newaxis]\n",
    "cm4_2 = cm4_2.astype('float') / cm4_2.sum(axis=1)[:, np.newaxis]\n",
    "cm4_3 = cm4_3.astype('float') / cm4_3.sum(axis=1)[:, np.newaxis]\n",
    "cm5 = cm5.astype('float') / cm5.sum(axis=1)[:, np.newaxis]\n",
    "cm6 = cm6.astype('float') / cm6.sum(axis=1)[:, np.newaxis]\n",
    "cm7 = cm7.astype('float') / cm7.sum(axis=1)[:, np.newaxis]\n",
    "cm8 = cm8.astype('float') / cm8.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "print '======================================'\n",
    "print 'Normalized C1', cm1,'\\n'\n",
    "print 'Normalized C2', cm2,'\\n'\n",
    "print 'Normalized C3', cm3,'\\n'\n",
    "print 'Normalized C3_DecTree', cm3_1,'\\n'\n",
    "print 'Normalized C3_Random_Forest', cm3_2,'\\n'\n",
    "print 'Normalized C3_Extra_Tree', cm3_3,'\\n'\n",
    "print 'Normalized C4', cm4,'\\n'\n",
    "print 'Normalized C4_AdaBoost', cm4_1,'\\n'\n",
    "print 'Normalized C4_gradiantBoost', cm4_2,'\\n'\n",
    "print 'Normalized C4_KNearestN',cm4_3,'\\n'\n",
    "print 'Normalized C5 Voting',cm5,'\\n'\n",
    "print 'Unormalized C6_NB_Gauss',cm6,'\\n'\n",
    "print 'Unormalized C7 NB_Bernulli',cm7,'\\n'\n",
    "print 'Unormalized C8 NB_Ensemble',cm8,'\\n'\n",
    "print '======================================'\n",
    "\n",
    "# Now we should write out the results of the training\n",
    "\n",
    "directory = 'model_files/'\n",
    "\n",
    "names = ['NB_MultiNomial.pkl','SVC.pkl','Tree_vote.pkl',\n",
    "         'DecTree.pkl','RandFor.pkl','Extree.pkl','otherM_vote.pkl',\n",
    "         'AdaB.pkl','gradB.pkl','KNneigh.pkl','tree_and_otherM_vote.pkl','NB_Gauss.pkl','NB_Bern.pkl','NB_vote.pkl']\n",
    "\n",
    "cm_names = ['NB_MultiNomial_cm.dat','SVC_cm.dat','Tree_vote_cm.dat',\n",
    "         'DecTree_cm.dat','RandFor_cm.dat','Extree_cm.dat','otherM_vote_cm.dat',\n",
    "         'AdaB_cm.dat','gradB_cm.dat','KNneigh_cm.dat','tree_and_otherM_vote_cm.dat','NB_Gauss_cm.dat','NB_Bern_cm.dat','NB_vote_cm.dat']\n",
    "\n",
    "model_array = [model1,model2,model3,m_DecTree,m_RandForest,m_ExTree,model4,\n",
    "                m_AdaBoost,m_gradBoost,m_KNneigh,model5,model6,model7,model8]  \n",
    "cm_array = [cm1,cm2,cm3,cm3_1,cm3_2,cm3_3,cm4,cm4_1,cm4_2,cm4_3,cm5,cm6,cm7,cm8]\n",
    "\n",
    "for k in range(len(names)):\n",
    "    name = directory+'dict_size_'+str(dictionary_size)+'_'+'dataSet_'+str(len(real_and_fake_df))+'_'+names[k]\n",
    "    with open(name, 'wb') as fid:\n",
    "        cPickle.dump(model_array[k], fid)\n",
    "\n",
    "# Write all cm to file\n",
    "for k in range(len(cm_names)):\n",
    "    cm_name = directory+'dict_size_'+str(dictionary_size)+'_'+'dataSet_'+str(len(real_and_fake_df))+'_'+cm_names[k]\n",
    "    f=open(cm_name,'w')\n",
    "    pickle.dump(cm_array[k], f)\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "print '====================================================================================='  \n",
    "print 'cm arrays saved to file'\n",
    "print 'pkl files saved to file'\n",
    "print 'Program Complete'\n",
    "print '====================================================================================='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
