{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Number:  18978\n",
      "Testing Set Number:  4745\n",
      "==============================================================\n",
      "The Size of the dictionary 119893\n",
      "Top Dictionary Words:  [('the', 1925), ('trump', 1016), ('to', 910), ('hillary', 691), ('in', 648), ('us', 634), ('new', 629), ('is', 616), ('clinton', 615), ('of', 513)]\n",
      "\n",
      "==============================================================\n",
      "array of labels size:  18978\n",
      "feature matrix:  18978\n",
      "=========================================\n",
      "Feature Matrix computation completed\n",
      "=========================================\n",
      "========================================================================================================================\n",
      "Fitting will now take place\n",
      "========================================================================================================================\n",
      "========================================================================================================================\n",
      "Fitting of Models is Complete\n",
      "========================================================================================================================\n",
      "========================================================================================================================\n",
      "Model Predictions on Test Set Complete\n",
      "========================================================================================================================\n",
      "======================================\n",
      "Unormalized C1 [[2208  167]\n",
      " [ 170 2200]] \n",
      "\n",
      "Unormalized C2 [[2183  192]\n",
      " [ 158 2212]] \n",
      "\n",
      "Unormalized C3 [[2231  144]\n",
      " [1545  825]] \n",
      "\n",
      "Unormalized C3_DecTree [[2208  167]\n",
      " [1563  807]] \n",
      "\n",
      "Unormalized C3_Random_Forest [[2221  154]\n",
      " [1080 1290]] \n",
      "\n",
      "Unormalized C3_Extra_Tree [[2206  169]\n",
      " [1006 1364]] \n",
      "\n",
      "Unormalized C4 [[1946  429]\n",
      " [ 190 2180]] \n",
      "\n",
      "Unormalized C4_AdaBoost [[1217 1158]\n",
      " [  42 2328]] \n",
      "\n",
      "Unormalized C4_gradiantBoost [[2264  111]\n",
      " [1286 1084]] \n",
      "\n",
      "Unormalized C4_KNearestN [[1945  430]\n",
      " [ 190 2180]] \n",
      "\n",
      "Unormalized C5_Voting [[2258  117]\n",
      " [ 934 1436]] \n",
      "\n",
      "Unormalized C6_NB_Gauss [[2204  171]\n",
      " [ 233 2137]] \n",
      "\n",
      "Unormalized C7 NB_Bernulli [[2203  172]\n",
      " [ 157 2213]] \n",
      "\n",
      "Unormalized C8 NB_Ensemble [[2205  170]\n",
      " [ 183 2187]] \n",
      "\n",
      "======================================\n",
      "======================================\n",
      "Normalized C1 [[ 0.92968421  0.07031579]\n",
      " [ 0.07172996  0.92827004]] \n",
      "\n",
      "Normalized C2 [[ 0.91915789  0.08084211]\n",
      " [ 0.06666667  0.93333333]] \n",
      "\n",
      "Normalized C3 [[ 0.93936842  0.06063158]\n",
      " [ 0.65189873  0.34810127]] \n",
      "\n",
      "Normalized C3_DecTree [[ 0.92968421  0.07031579]\n",
      " [ 0.65949367  0.34050633]] \n",
      "\n",
      "Normalized C3_Random_Forest [[ 0.93515789  0.06484211]\n",
      " [ 0.4556962   0.5443038 ]] \n",
      "\n",
      "Normalized C3_Extra_Tree [[ 0.92884211  0.07115789]\n",
      " [ 0.42447257  0.57552743]] \n",
      "\n",
      "Normalized C4 [[ 0.81936842  0.18063158]\n",
      " [ 0.08016878  0.91983122]] \n",
      "\n",
      "Normalized C4_AdaBoost [[ 0.51242105  0.48757895]\n",
      " [ 0.01772152  0.98227848]] \n",
      "\n",
      "Normalized C4_gradiantBoost [[ 0.95326316  0.04673684]\n",
      " [ 0.54261603  0.45738397]] \n",
      "\n",
      "Normalized C4_KNearestN [[ 0.81894737  0.18105263]\n",
      " [ 0.08016878  0.91983122]] \n",
      "\n",
      "Normalized C5 Voting [[ 0.95073684  0.04926316]\n",
      " [ 0.39409283  0.60590717]] \n",
      "\n",
      "Unormalized C6_NB_Gauss [[ 0.928       0.072     ]\n",
      " [ 0.09831224  0.90168776]] \n",
      "\n",
      "Unormalized C7 NB_Bernulli [[ 0.92757895  0.07242105]\n",
      " [ 0.06624473  0.93375527]] \n",
      "\n",
      "Unormalized C8 NB_Ensemble [[ 0.92842105  0.07157895]\n",
      " [ 0.07721519  0.92278481]] \n",
      "\n",
      "======================================\n",
      "=====================================================================================\n",
      "cm arrays saved to file\n",
      "pkl files saved to file\n",
      "Program Complete\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "#import sys \n",
    "#reload(sys) \n",
    "#sys.setdefaultencoding('utf8')\n",
    "import cPickle\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import numerical_routines as nr\n",
    "import collections \n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "# Import a lot of models...\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import random\n",
    "\n",
    "# Import Tree Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "# Here we read in the data-set of real and fake news items\n",
    "real_and_fake_df = pd.read_csv('real-and-fake-news-titles/fake_and_real_news_titles.csv')\n",
    "real_and_fake_df.head()\n",
    "\n",
    "# Take a subset of the data\n",
    "# 23,723\n",
    "#real_and_fake_df = real_and_fake_df[0:500]\n",
    "\n",
    "# The size of the dictionary array vectors\n",
    "dictionary_size = 3000\n",
    "\n",
    "\n",
    "# Now we split the data into the training set and test data set\n",
    "split_frac = 0.8\n",
    "data_set_size = len(real_and_fake_df)\n",
    "split_indx = int(data_set_size*split_frac)\n",
    "\n",
    "training_df = real_and_fake_df[:split_indx]\n",
    "testing_df = real_and_fake_df[split_indx:]\n",
    "\n",
    "training_size = len(training_df)\n",
    "testing_size = len(testing_df)\n",
    "\n",
    "print 'Training Set Number: ', training_size\n",
    "print 'Testing Set Number: ', testing_size\n",
    "\n",
    "# Now the data from the story titles is extracted into one large array containing all words in the title\n",
    "title_words_array = nr.aggregate_data(training_df.dropna(subset=['title']),size= training_size)\n",
    "#print title_words_array\n",
    "# Now we make filter the title words \n",
    "filtered_title_words_array = nr.create_dictionary(title_words_array)\n",
    "\n",
    "\n",
    "# Now we count the frequency of the different words\n",
    "dictionary = collections.Counter(filtered_title_words_array)\n",
    "\n",
    "list_to_remove = dictionary.keys()\n",
    "\n",
    "# Remove one character items \n",
    "for key in list_to_remove:\n",
    "    \n",
    "    if len(key) == 1:\n",
    "        del dictionary[key]\n",
    "\n",
    "# Now take a reasonable subset of the data\n",
    "dictionary = dictionary.most_common(dictionary_size)\n",
    "\n",
    "print '=============================================================='\n",
    "print 'The Size of the dictionary', len(filtered_title_words_array)\n",
    "print 'Top Dictionary Words: ', dictionary[0:10] \n",
    "print ''\n",
    "print '=============================================================='\n",
    "\n",
    "# We convert a data set into a feature matrix\n",
    "feature_matrix = nr.create_featureMatrix(training_df,dictionary,size= training_size)\n",
    "\n",
    "# We find the labels of the data that we converted into the feature matrix and convert into an array\n",
    "label_array = nr.create_label_array(training_df,size= training_size)\n",
    "\n",
    "print 'array of labels size: ', len(label_array)\n",
    "print 'feature matrix: ', len(feature_matrix)\n",
    "\n",
    "print '========================================='\n",
    "print 'Feature Matrix computation completed'\n",
    "print '========================================='\n",
    "\n",
    "\n",
    "\n",
    "# Now we train the various classifier\n",
    "model1 = MultinomialNB()\n",
    "model2 = LinearSVC()\n",
    "\n",
    "# Model\n",
    "m_DecTree = DecisionTreeClassifier(max_depth=5)\n",
    "m_RandForest = RandomForestClassifier(max_depth=5, n_estimators=50)\n",
    "m_ExTree = ExtraTreesClassifier(max_depth=5, n_estimators=50)\n",
    "\n",
    "\n",
    "print '========================================================================================================================'\n",
    "print 'Fitting will now take place'\n",
    "print '========================================================================================================================'\n",
    "\n",
    "\n",
    "model1.fit(feature_matrix,label_array)\n",
    "model2.fit(feature_matrix,label_array)\n",
    "mixingWeights = [1.0,1.0,1.0]\n",
    "numOfNeighs = 1 \n",
    "\n",
    "#model3 = VotingClassifier(estimators=[ ('dtc', clf1), ('rfc', clf2), ('etc', clf3) ], voting='soft', weights= mixingWeights )\n",
    "model3 = VotingClassifier(estimators=[ ('dtc', m_DecTree), ('rfc', m_RandForest), ('etc', m_ExTree) ], voting='soft', weights= mixingWeights )\n",
    "model3.fit(feature_matrix, label_array)\n",
    "\n",
    "# fit models individually\n",
    "m_DecTree.fit(feature_matrix, label_array)\n",
    "m_RandForest.fit(feature_matrix, label_array)\n",
    "m_ExTree.fit(feature_matrix, label_array)\n",
    "\n",
    "# More Models\n",
    "#clf4 = AdaBoostClassifier(n_estimators=50)\n",
    "#clf5 = GradientBoostingClassifier(n_estimators=50)\n",
    "#clf6 = KNeighborsClassifier(n_neighbors=numOfNeighs)\n",
    "\n",
    "# More Models\n",
    "m_AdaBoost = AdaBoostClassifier(n_estimators=50)\n",
    "m_gradBoost = GradientBoostingClassifier(n_estimators=50)\n",
    "m_KNneigh = KNeighborsClassifier(n_neighbors=numOfNeighs)\n",
    "\n",
    "model4 = VotingClassifier(estimators=[ ('dtc', m_AdaBoost), ('rfc', m_gradBoost), ('etc', m_KNneigh) ], voting='soft', weights= mixingWeights )\n",
    "model4.fit(feature_matrix, label_array)\n",
    "\n",
    "estimator_models = [ ('dtc', m_DecTree), ('rfc', m_RandForest), ('etc', m_ExTree),('dtc', m_AdaBoost), ('rfc', m_gradBoost), ('etc', m_KNneigh),\n",
    "                   ('MNB',model1)]\n",
    "model5 = VotingClassifier(estimators=estimator_models, voting='hard')\n",
    "model5.fit(feature_matrix, label_array)\n",
    "\n",
    "\n",
    "model6 = GaussianNB()\n",
    "model7 = BernoulliNB()\n",
    "\n",
    "model6.fit(feature_matrix,label_array)\n",
    "model7.fit(feature_matrix,label_array)\n",
    "\n",
    "\n",
    "estimator_models = [ ('m1', model1), ('m6', model6), ('m7', model7)]\n",
    "model8 = VotingClassifier(estimators=estimator_models, voting='soft')\n",
    "model8.fit(feature_matrix, label_array)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit the extra models\n",
    "m_AdaBoost.fit(feature_matrix, label_array)\n",
    "m_gradBoost.fit(feature_matrix, label_array)\n",
    "m_KNneigh.fit(feature_matrix, label_array)\n",
    "\n",
    "print '========================================================================================================================'\n",
    "print 'Fitting of Models is Complete'\n",
    "print '========================================================================================================================'\n",
    "\n",
    "#========================================================================================================================\n",
    "#========================================================================================================================\n",
    "# Now we make a prediction on the testing data set\n",
    "feature_matrix_test = nr.create_featureMatrix(testing_df,dictionary, size=testing_size)\n",
    "label_array_test = nr.create_label_array(testing_df,size= testing_size)\n",
    "\n",
    "# Calculate the results of all of the models\n",
    "result1 = model1.predict(feature_matrix_test)\n",
    "result2 = model2.predict(feature_matrix_test)\n",
    "\n",
    "# Make Predictions with the second ensemble models\n",
    "\n",
    "\n",
    "# Make Predictions with the third ensemble models\n",
    "result3 = model3.predict(feature_matrix_test)\n",
    "result3_1 = m_DecTree.predict(feature_matrix_test)\n",
    "result3_2 = m_RandForest.predict(feature_matrix_test)\n",
    "result3_3 = m_ExTree.predict(feature_matrix_test)\n",
    "\n",
    "result4 = model4.predict(feature_matrix_test)\n",
    "result4_1 = m_AdaBoost.predict(feature_matrix_test)\n",
    "result4_2 = m_gradBoost.predict(feature_matrix_test)\n",
    "result4_3 = m_KNneigh.predict(feature_matrix_test)\n",
    "\n",
    "result5 = model5.predict(feature_matrix_test)\n",
    "\n",
    "result6 = model6.predict(feature_matrix_test)\n",
    "result7 = model7.predict(feature_matrix_test)\n",
    "result8 = model8.predict(feature_matrix_test)\n",
    "\n",
    "print '========================================================================================================================'\n",
    "print 'Model Predictions on Test Set Complete'\n",
    "print '========================================================================================================================'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cm1 = confusion_matrix(label_array_test,result1)\n",
    "cm2 = confusion_matrix(label_array_test,result2)\n",
    "cm3 = confusion_matrix(label_array_test,result3)\n",
    "cm3_1 = confusion_matrix(label_array_test,result3_1)\n",
    "cm3_2 = confusion_matrix(label_array_test,result3_2)\n",
    "cm3_3 = confusion_matrix(label_array_test,result3_3)\n",
    "\n",
    "cm4 = confusion_matrix(label_array_test,result4)\n",
    "cm4_1 = confusion_matrix(label_array_test,result4_1)\n",
    "cm4_2 = confusion_matrix(label_array_test,result4_2)\n",
    "cm4_3 = confusion_matrix(label_array_test,result4_3)\n",
    "cm5 = confusion_matrix(label_array_test,result5)\n",
    "\n",
    "cm6 = confusion_matrix(label_array_test,result6)\n",
    "cm7 = confusion_matrix(label_array_test,result7)\n",
    "cm8 = confusion_matrix(label_array_test,result8)\n",
    "\n",
    "print '======================================'\n",
    "print 'Unormalized C1', cm1,'\\n'\n",
    "print 'Unormalized C2', cm2,'\\n'\n",
    "print 'Unormalized C3', cm3,'\\n'\n",
    "print 'Unormalized C3_DecTree', cm3_1,'\\n'\n",
    "print 'Unormalized C3_Random_Forest', cm3_2,'\\n'\n",
    "print 'Unormalized C3_Extra_Tree', cm3_3,'\\n'\n",
    "print 'Unormalized C4', cm4,'\\n'\n",
    "print 'Unormalized C4_AdaBoost', cm4_1,'\\n'\n",
    "print 'Unormalized C4_gradiantBoost', cm4_2,'\\n'\n",
    "print 'Unormalized C4_KNearestN',cm4_3,'\\n'\n",
    "print 'Unormalized C5_Voting',cm5,'\\n'\n",
    "print 'Unormalized C6_NB_Gauss',cm6,'\\n'\n",
    "print 'Unormalized C7 NB_Bernulli',cm7,'\\n'\n",
    "print 'Unormalized C8 NB_Ensemble',cm8,'\\n'\n",
    "print '======================================'\n",
    "\n",
    "# Now we print off the Confusion matrix for the models\n",
    "cm1 = cm1.astype('float') / cm1.sum(axis=1)[:, np.newaxis]\n",
    "cm2 = cm2.astype('float') / cm2.sum(axis=1)[:, np.newaxis]\n",
    "cm3 = cm3.astype('float') / cm3.sum(axis=1)[:, np.newaxis]\n",
    "cm3_1 = cm3_1.astype('float') / cm3_1.sum(axis=1)[:, np.newaxis]\n",
    "cm3_2 = cm3_2.astype('float') / cm3_2.sum(axis=1)[:, np.newaxis]\n",
    "cm3_3 = cm3_3.astype('float') / cm3_3.sum(axis=1)[:, np.newaxis]\n",
    "cm4 = cm4.astype('float') / cm4.sum(axis=1)[:, np.newaxis]\n",
    "cm4_1 = cm4_1.astype('float') / cm4_1.sum(axis=1)[:, np.newaxis]\n",
    "cm4_2 = cm4_2.astype('float') / cm4_2.sum(axis=1)[:, np.newaxis]\n",
    "cm4_3 = cm4_3.astype('float') / cm4_3.sum(axis=1)[:, np.newaxis]\n",
    "cm5 = cm5.astype('float') / cm5.sum(axis=1)[:, np.newaxis]\n",
    "cm6 = cm6.astype('float') / cm6.sum(axis=1)[:, np.newaxis]\n",
    "cm7 = cm7.astype('float') / cm7.sum(axis=1)[:, np.newaxis]\n",
    "cm8 = cm8.astype('float') / cm8.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "print '======================================'\n",
    "print 'Normalized C1', cm1,'\\n'\n",
    "print 'Normalized C2', cm2,'\\n'\n",
    "print 'Normalized C3', cm3,'\\n'\n",
    "print 'Normalized C3_DecTree', cm3_1,'\\n'\n",
    "print 'Normalized C3_Random_Forest', cm3_2,'\\n'\n",
    "print 'Normalized C3_Extra_Tree', cm3_3,'\\n'\n",
    "print 'Normalized C4', cm4,'\\n'\n",
    "print 'Normalized C4_AdaBoost', cm4_1,'\\n'\n",
    "print 'Normalized C4_gradiantBoost', cm4_2,'\\n'\n",
    "print 'Normalized C4_KNearestN',cm4_3,'\\n'\n",
    "print 'Normalized C5 Voting',cm5,'\\n'\n",
    "print 'Unormalized C6_NB_Gauss',cm6,'\\n'\n",
    "print 'Unormalized C7 NB_Bernulli',cm7,'\\n'\n",
    "print 'Unormalized C8 NB_Ensemble',cm8,'\\n'\n",
    "print '======================================'\n",
    "\n",
    "# Now we should write out the results of the training\n",
    "\n",
    "\n",
    "# Now that we've had our fun, save all of the models to disk\n",
    "\n",
    "\n",
    "#names = ['NB_MultiNomial.pkl','SVC.pkl','Tree_vote.pkl','NB_Gauss.pkl','NB_Bern.pkl','NB_vote.pkl']\n",
    "#model_array = [model1,model2,model3,model6,model7,model8]         \n",
    "\n",
    "\n",
    "\n",
    "names = ['NB_MultiNomial.pkl','SVC.pkl','Tree_vote.pkl',\n",
    "         'DecTree.pkl','RandFor.pkl','Extree.pkl','otherM_vote.pkl',\n",
    "         'AdaB.pkl','gradB.pkl','KNneigh.pkl','tree_and_otherM_vote.pkl','NB_Gauss.pkl','NB_Bern.pkl','NB_vote.pkl']\n",
    "\n",
    "cm_names = ['NB_MultiNomial_cm.dat','SVC_cm.dat','Tree_vote_cm.dat',\n",
    "         'DecTree_cm.dat','RandFor_cm.dat','Extree_cm.dat','otherM_vote_cm.dat',\n",
    "         'AdaB_cm.dat','gradB_cm.dat','KNneigh_cm.dat','tree_and_otherM_vote_cm.dat','NB_Gauss_cm.dat','NB_Bern_cm.dat','NB_vote_cm.dat']\n",
    "\n",
    "model_array = [model1,model2,model3,m_DecTree,m_RandForest,m_ExTree,model4,\n",
    "                m_AdaBoost,m_gradBoost,m_KNneigh,model5,model6,model7,model8]  \n",
    "cm_array = [cm1,cm2,cm3,cm3_1,cm3_2,cm3_3,cm4,cm4_1,cm4_2,cm4_3,cm5,cm6,cm7,cm8]\n",
    "\n",
    "for k in range(len(names)):\n",
    "    name = 'dict_size_'+str(dictionary_size)+'_'+'dataSet_'+str(len(real_and_fake_df))+'_'+names[k]\n",
    "    with open(name, 'wb') as fid:\n",
    "        cPickle.dump(model_array[k], fid)\n",
    "\n",
    "# Write all cm to file\n",
    "for k in range(len(cm_names)):\n",
    "    cm_name = 'dict_size_'+str(dictionary_size)+'_'+'dataSet_'+str(len(real_and_fake_df))+'_'+cm_names[k]\n",
    "    f=open(cm_name,'w')\n",
    "    pickle.dump(cm_array[k], f)\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "print '====================================================================================='  \n",
    "print 'cm arrays saved to file'\n",
    "print 'pkl files saved to file'\n",
    "print 'Program Complete'\n",
    "print '====================================================================================='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
